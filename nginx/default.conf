worker_processes auto;                 # Define quantos processos de worker o Nginx vai criar. "auto" faz o Nginx detectar o nº de CPUs e criar 1 worker por CPU, o que maximiza paralelismo sem oversubscription.

worker_rlimit_nofile 1048576;          # Aumenta o limite de descritores de arquivo (FDs) que cada worker pode abrir (precisa combinar com ulimit/limits do SO). Essencial quando há muitas conexões simultâneas e sockets (cada conexão consome 1 FD).

events {                                # Bloco de configurações do loop de eventos (multiplexação de conexões) dos workers.
  use epoll;                           # Em Linux, usa o backend "epoll" (altamente escalável) para I/O não bloqueante e grande número de conexões.
  worker_connections 65535;            # Nº máx. de conexões simultâneas por worker. Máx. de clientes ≈ worker_processes * worker_connections (menos conexões internas/upstream).
  multi_accept on;                     # Faz o worker aceitar múltiplas conexões pendentes por ciclo, melhorando taxa de accept() em pico.
  accept_mutex off;                    # Desabilita o "lock" de accept; com 'reuseport' (no 'listen'), todos os workers aceitam em paralelo, reduzindo head-of-line.
}

http {                                  # Bloco de configuração HTTP (proxy, timeouts, buffers, etc.).
  # I/O e TCP
  sendfile on;                          # Ativa envio de arquivos direto do kernel (zero-copy). Mesmo para API (pouco estático), não prejudica e pode otimizar headers/corpos pequenos.
  tcp_nopush on;                        # Com sendfile, empacota headers em segmentos maiores antes de enviar (reduz overhead de pacotes).
  tcp_nodelay on;                       # Desliga Nagle: envia pequenos pacotes imediatamente (melhor latência para respostas curtas típicas de API).
  aio threads=default;                  # Ativa AIO com thread pool para operações de arquivo. Para proxy puro quase não é usado, mas é seguro manter.

  server_tokens off;                    # Oculta versão do Nginx nas respostas (melhora segurança por obscurecimento).
  access_log off;                       # Desabilita log de acesso (reduz I/O e CPU). Útil em benchmark para medir só o custo de proxy.

  # mate slow-clients rápido (benchmark não é slow-loris)
  client_header_timeout 5s;             # Tempo máx. para o cliente enviar os headers; clientes lentos são desconectados.
  client_body_timeout   5s;             # Tempo máx. para o corpo da requisição; evita ficar segurando conexões de clientes lentos.
  send_timeout          5s;             # Tempo máx. sem atividade durante envio de resposta ao cliente; expira conexões presas.

  # keep-alive cliente <-> nginx (reduz handshakes e explode RPS)
  keepalive_timeout   15s;              # Mantém conexões do cliente abertas por até 15s ociosas, reduzindo custo de novos handshakes TCP/TLS em cargas altas.
  keepalive_requests  100000;           # Nº máx. de requisições por conexão keep-alive do cliente. Alto para evitar fechamentos frequentes em benchmark.

  # defaults de proxy para API
  proxy_http_version 1.1;               # Usa HTTP/1.1 para upstreams (necessário para keep-alive, chunked, etc.).
  proxy_buffering off;                  # Não armazena resposta em buffers; faz streaming direto para o cliente (menor latência, menos cópias; pode reduzir throughput em corpos grandes).
  proxy_request_buffering off;          # Não bufferiza o corpo do request em disco/memória antes de enviar ao upstream; envia em streaming (bom p/ POSTs grandes e baixa latência).
  proxy_max_temp_file_size 0;           # Garante que não haverá arquivos temporários de proxy; força tudo a ir em memória/streaming.

  proxy_connect_timeout 1s;             # Tempo para estabelecer conexão com o upstream (app). Curto para falhar rápido sob saturação.
  proxy_send_timeout    5s;             # Tempo máx. para enviar request ao upstream; evita pendurar em app lento.
  proxy_read_timeout    5s;             # Tempo máx. de espera entre pacotes da resposta do upstream; protege contra travas/pausas longas.

  # encaminhar headers úteis
  proxy_set_header Host              $host;                      # Preserva o Host original (útil p/ apps que dependem de virtual host/links).
  proxy_set_header X-Real-IP         $remote_addr;               # IP do cliente direto (para logs/ratelimiting no app).
  proxy_set_header X-Forwarded-For   $proxy_add_x_forwarded_for; # Encadeia IPs do cliente/proxies anteriores no cabeçalho padrão.
  proxy_set_header X-Forwarded-Proto $scheme;                    # Informa ao upstream se o acesso original foi HTTP/HTTPS.

  upstream fastify_servers {                           # Define o pool/estratégia de balanceamento para os servidores Fastify.
    least_conn;                                        # Algoritmo "menor nº de conexões ativas": distribui melhor quando há variação de latência/tamanho de requisição.
    server app1:3001 max_fails=0;                      # Nó 1 do upstream (desabilita contagem de falhas para não marcar como down em benchmark; útil quando o app está sob carga extrema).
    server app2:3002 max_fails=0;                      # Nó 2 do upstream (mesma observação).
                                                       # OBS: Em produção, normalmente usa-se 'max_fails' e 'fail_timeout' para remoção temporária de nós ruins.

    # pool de conexões persistentes nginx -> fastify (evita exaustão de portas)
    keepalive 2048;                   # Mantém até 2048 conexões *ociosas* por worker com o upstream. Reuso reduz TIME_WAIT e custo de 3-way handshake, melhora RPS e evita esgotar portas efêmeras.
  }

  server {                                              # Bloco do servidor virtual (vhost) que aceitará conexões dos clientes.
    listen 9999 reuseport backlog=65535;  # Porta pública. 'reuseport' cria um socket por worker e o kernel distribui conexões entre eles (melhor paralelismo).
                                          # 'backlog=65535' aumenta a fila de conexões pendentes (sincronia com 'net.core.somaxconn' no sysctl).

    location / {                                         # Rota padrão: todas as requisições vão para o upstream.
      proxy_pass http://fastify_servers;                 # Encaminha a requisição ao grupo 'fastify_servers' definido acima (balanceado).

      # necessário p/ upstream keepalive funcionar corretamente
      proxy_http_version 1.1;                            # Garante uso de HTTP/1.1 também aqui (entre Nginx e upstream) para permitir conexões persistentes.
      proxy_set_header Connection "";                    # Remove header "Connection" explícito, permitindo que o Nginx gerencie keep-alive com o upstream (evita "Connection: close").
    }
  }
}
